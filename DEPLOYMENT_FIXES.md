# ğŸ”§ Deployment Fixes & Improvements

TÃ i liá»‡u nÃ y mÃ´ táº£ chi tiáº¿t cÃ¡c váº¥n Ä‘á» gáº·p pháº£i trong quÃ¡ trÃ¬nh deployment vÃ  cÃ¡ch kháº¯c phá»¥c.

---

## ğŸ“‹ Tá»•ng Quan

**NgÃ y:** 30/01/2026  
**PhiÃªn báº£n:** v1.0.32+  
**Má»¥c tiÃªu:** Tá»± Ä‘á»™ng hÃ³a hoÃ n toÃ n quy trÃ¬nh CI/CD deployment cho há»‡ thá»‘ng Landslide Monitoring

---

## âŒ CÃ¡c Váº¥n Äá» ÄÃ£ Gáº·p

### 1. Container Conflict Issues
**Triá»‡u chá»©ng:**
```
Error: The container name "/mosquitto" is already in use
Error: The container name "/mqtt-bridge" is already in use
```

**NguyÃªn nhÃ¢n:**
- Gateway vÃ  Server deploy riÃªng biá»‡t nhÆ°ng cÃ³ containers trÃ¹ng tÃªn
- CI/CD deployment khÃ´ng cleanup containers cÅ© trÆ°á»›c khi táº¡o má»›i

**Kháº¯c phá»¥c:**
- ThÃªm step cleanup toÃ n diá»‡n trÆ°á»›c khi deploy
- XÃ³a táº¥t cáº£ containers: `mosquitto`, `mqtt-bridge`, `kafka`, `spark-master`, `spark-worker`, `zookeeper`

### 2. Permission Denied Issues
**Triá»‡u chá»©ng:**
```
Error: EACCES: permission denied, unlink '/mosquitto/log/mosquitto.log'
```

**NguyÃªn nhÃ¢n:**
- Docker containers táº¡o files vá»›i user `root`
- GitHub Actions runner (user `dungtm`) khÃ´ng cÃ³ quyá»n xÃ³a

**Kháº¯c phá»¥c:**
```bash
# ThÃªm vÃ o /etc/sudoers.d/github-runner
dungtm ALL=(ALL) NOPASSWD: /bin/rm, /bin/chown
```

### 3. Network Isolation Issues
**Triá»‡u chá»©ng:**
```
>>> Bridge: Cho Kafka khoi dong... (NoBrokersAvailable)
```

**NguyÃªn nhÃ¢n:**
- Gateway (mosquitto, mqtt-bridge) vÃ  Server (kafka, spark) trong 2 networks riÃªng
- `mqtt-bridge` khÃ´ng thá»ƒ resolve hostname `kafka:29092`

**Kháº¯c phá»¥c:**
- Táº¡o shared Docker network: `landslide_network`
- Táº¥t cáº£ containers Ä‘á»u join network nÃ y

### 4. Missing Spark Jobs Volume Mount
**Triá»‡u chá»©ng:**
```
ls: cannot access '/app/spark_jobs/': No such file or directory
```

**NguyÃªn nhÃ¢n:**
- `processor.py` náº±m á»Ÿ host nhÆ°ng khÃ´ng Ä‘Æ°á»£c mount vÃ o spark-master container

**Kháº¯c phá»¥c:**
```yaml
spark-master:
  volumes:
    - ./spark_jobs:/app/spark_jobs
```

### 5. Spark Batch Data Not Visible
**Triá»‡u chá»©ng:**
- Spark job cháº¡y nhÆ°ng khÃ´ng tháº¥y batch output (báº£ng ASCII)

**NguyÃªn nhÃ¢n:**
- Sá»­ dá»¥ng `docker exec -d` (detached mode) â†’ console output bá»‹ máº¥t
- `.format("console")` output khÃ´ng Ä‘Æ°á»£c capture

**Kháº¯c phá»¥c:**
```bash
# Redirect output vÃ o file
docker exec spark-master bash -c "nohup /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  /app/spark_jobs/processor.py \
  > /app/spark_jobs/spark_output.log 2>&1 &"
```

---

## âœ… CÃ¡c Thay Äá»•i ÄÃ£ Thá»±c Hiá»‡n

### 1. Docker Compose Files

#### `gateway/docker-compose.yml`
```yaml
services:
  mosquitto:
    networks:
      - landslide_network
  
  mqtt-bridge:
    networks:
      - landslide_network

networks:
  landslide_network:
    external: true
    name: landslide_network
```

#### `server/docker-compose.yml`
```yaml
services:
  kafka:
    networks:
      - landslide_network
  
  spark-master:
    volumes:
      - ./spark_jobs:/app/spark_jobs
    networks:
      - landslide_network
  
  spark-worker:
    networks:
      - landslide_network
  
  zookeeper:
    networks:
      - landslide_network

networks:
  landslide_network:
    external: true
    name: landslide_network
```

### 2. CI/CD Pipeline (`ci-cd-pipeline.yml`)

#### BÆ°á»›c 1: Cleanup vá»›i Permission Fix
```yaml
- name: Clean Workspace (Fix Permission Issues)
  run: |
    sudo rm -rf "$WORKSPACE_DIR/gateway/mosquitto/log"
    sudo chown -R $(whoami):$(whoami) "$WORKSPACE_DIR"
```

#### BÆ°á»›c 2: Táº¡o Shared Network
```yaml
docker network create landslide_network
```

#### BÆ°á»›c 3: Deploy Gateway â†’ Server
```yaml
# Gateway trÆ°á»›c
cd gateway
docker-compose up -d

# Server sau
cd ../server
docker-compose up -d
```

#### BÆ°á»›c 4: Submit Spark Job vá»›i Output Redirection
```yaml
docker exec spark-master bash -c "nohup /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  /app/spark_jobs/processor.py \
  > /app/spark_jobs/spark_output.log 2>&1 &"
```

#### BÆ°á»›c 5: Wait 5 Minutes vá»›i Progress Indicator
```yaml
for i in {1..10}; do
  ELAPSED=$((i * 30))
  echo "Progress: ${ELAPSED}s / 300s"
  sleep 30
done
```

#### BÆ°á»›c 6: Collect Logs vá»›i Batch Data
```yaml
docker exec spark-master cat /app/spark_jobs/spark_output.log | grep -B 3 -A 10 "Batch:"
```

---

## ğŸš€ Quy TrÃ¬nh Deployment Má»›i

### Tá»± Äá»™ng (CI/CD)
```bash
# Push code vá»›i tag
git add .
git commit -m "Your changes"
git tag v1.0.32
git push origin main
git push origin v1.0.32
```

**Thá»i gian:** ~7-8 phÃºt (bao gá»“m 5 phÃºt Ä‘á»£i data flow)

**Káº¿t quáº£:**
- âœ… 6 containers running: mosquitto, mqtt-bridge, kafka, zookeeper, spark-master, spark-worker
- âœ… Shared network: landslide_network
- âœ… Spark job processing batches
- âœ… Log file: `~/spark_logs/spark_deployment_[timestamp].log`

### Thá»§ CÃ´ng (Troubleshooting)

#### 1. Cleanup HoÃ n ToÃ n
```bash
# XÃ³a containers
docker rm -f mosquitto mqtt-bridge kafka spark-master spark-worker zookeeper

# XÃ³a network
docker network rm landslide_network

# Cleanup files
sudo rm -rf ~/landslide-server/spark_jobs/checkpoint
sudo rm -rf ~/landslide-server/gateway/mosquitto/log
```

#### 2. Deploy Tá»« Äáº§u
```bash
# Táº¡o network
docker network create landslide_network

# Deploy gateway
cd ~/landslide-server/gateway
docker-compose up -d

# Deploy server
cd ~/landslide-server/server
docker-compose up -d

# Submit Spark job
docker exec spark-master bash -c "nohup /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  /app/spark_jobs/processor.py \
  > /app/spark_jobs/spark_output.log 2>&1 &"

# Äá»£i vÃ  xem batch data
sleep 60
docker exec spark-master tail -f /app/spark_jobs/spark_output.log
```

---

## ğŸ“Š Kiá»ƒm Tra Káº¿t Quáº£

### 1. Container Status
```bash
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
```

**Expected:**
```
NAMES          STATUS         PORTS
mosquitto      Up X minutes   0.0.0.0:1883->1883/tcp, 0.0.0.0:9001->9001/tcp
mqtt-bridge    Up X minutes   
kafka          Up X minutes   0.0.0.0:9092->9092/tcp
zookeeper      Up X minutes   2181/tcp, 2888/tcp, 3888/tcp
spark-master   Up X minutes   0.0.0.0:7077->7077/tcp, 0.0.0.0:9090->8080/tcp
spark-worker   Up X minutes   
```

### 2. Network Connectivity
```bash
docker network inspect landslide_network | grep Name
```

**Expected:** Táº¥t cáº£ 6 containers trong cÃ¹ng network

### 3. MQTT Bridge â†’ Kafka
```bash
docker logs mqtt-bridge --tail 20
```

**Expected:**
```
>>> Bridge: Da ket noi Kafka thanh cong!
 -> Da chuyen ti: sensors/cluster_01/rain
 -> Da chuyen ti: sensors/cluster_01/imu
```

### 4. Kafka Messages
```bash
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic landslide_data \
  --from-beginning \
  --max-messages 5
```

**Expected:** JSON messages tá»« sensors

### 5. Spark Batch Processing
```bash
docker exec spark-master cat /app/spark_jobs/spark_output.log | grep -A 10 "Batch:"
```

**Expected:**
```
-------------------------------------------
Batch: 0
-------------------------------------------
+------------------------------------------+---------+--------+---------------+---------+
|window                                    |Avg_Acc_Z|Max_Rain|Max_Water_Level|Last_GNSS|
+------------------------------------------+---------+--------+---------------+---------+
|{2026-01-30 09:00:00, 2026-01-30 09:00:10}|-9.8123  |4.2     |2.45           |105.8542 |
+------------------------------------------+---------+--------+---------------+---------+
```

### 6. Deployment Log
```bash
ls -lt ~/spark_logs/ | head -2
cat ~/spark_logs/spark_deployment_*.log | grep "SUCCESS"
```

---

## ğŸ” Troubleshooting

### Issue: KhÃ´ng tháº¥y batch data
**Check:**
```bash
# 1. Spark job cÃ³ cháº¡y khÃ´ng?
docker exec spark-master ps aux | grep processor.py

# 2. Output file cÃ³ tá»“n táº¡i khÃ´ng?
docker exec spark-master ls -lh /app/spark_jobs/spark_output.log

# 3. CÃ³ lá»—i trong output khÃ´ng?
docker exec spark-master tail -100 /app/spark_jobs/spark_output.log | grep -i error

# 4. Kafka cÃ³ data khÃ´ng?
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic landslide_data \
  --from-beginning \
  --max-messages 1
```

**Fix:**
```bash
# Restart Spark job
docker exec spark-master pkill -f processor.py
docker exec spark-master rm -rf /app/spark_jobs/checkpoint
docker exec spark-master bash -c "nohup /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  /app/spark_jobs/processor.py \
  > /app/spark_jobs/spark_output.log 2>&1 &"
```

### Issue: MQTT Bridge khÃ´ng connect Kafka
**Check:**
```bash
docker logs mqtt-bridge | grep -i kafka
docker network inspect landslide_network | grep mqtt-bridge
```

**Fix:**
```bash
# Recreate mqtt-bridge trong Ä‘Ãºng network
docker rm -f mqtt-bridge
cd ~/landslide-server/gateway
docker-compose up -d mqtt-bridge
```

### Issue: Permission denied khi deploy
**Check:**
```bash
sudo -l | grep dungtm
```

**Fix:**
```bash
sudo nano /etc/sudoers.d/github-runner
# ThÃªm: dungtm ALL=(ALL) NOPASSWD: /bin/rm, /bin/chown
sudo chmod 440 /etc/sudoers.d/github-runner
```

---

## ğŸ“ˆ Metrics & Monitoring

### CI/CD Pipeline Duration
- **Build & Push:** ~2-3 phÃºt
- **Deploy:** ~5-6 phÃºt (bao gá»“m 5 phÃºt wait time)
- **Total:** ~7-9 phÃºt

### Resource Usage
```bash
docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}"
```

### Batch Processing Rate
```bash
# Äáº¿m sá»‘ batches trong 5 phÃºt
docker exec spark-master grep -c "Batch:" /app/spark_jobs/spark_output.log
```

**Expected:** ~30-50 batches/5 minutes (10s window)

---

## ğŸ“ Checklist TrÆ°á»›c Khi Deploy

- [ ] Edge sensors Ä‘ang cháº¡y
- [ ] Server cÃ³ quyá»n sudo cho user `dungtm`
- [ ] GitHub Actions runner Ä‘ang hoáº¡t Ä‘á»™ng
- [ ] Ports available: 1883, 9001, 9092, 7077, 9090
- [ ] Disk space Ä‘á»§ cho logs vÃ  checkpoints
- [ ] Code Ä‘Ã£ Ä‘Æ°á»£c test locally

---

## ğŸ¯ Best Practices

1. **Always use tags:** `v1.0.x` format
2. **Monitor first deployment:** Check logs manually
3. **Keep cleanup steps:** Don't skip permission fixes
4. **Use shared network:** Ensure all services communicate
5. **Redirect Spark output:** Never use detached mode without logging
6. **Wait for data flow:** 5 minutes minimum for meaningful batch data

---

## ğŸ”— Related Files

- `.github/workflows/ci-cd-pipeline.yml` - CI/CD configuration
- `gateway/docker-compose.yml` - Gateway services
- `server/docker-compose.yml` - Server services
- `server/spark_jobs/processor.py` - Spark streaming job

---

## âœï¸ Authors & Contributors

**Fixed by:** AI Assistant  
**Tested on:** Server `apache-server` (ubuntu)  
**Date:** 30/01/2026

---

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á», check:
1. Deployment log: `~/spark_logs/spark_deployment_*.log`
2. Container logs: `docker logs <container_name>`
3. Spark output: `docker exec spark-master cat /app/spark_jobs/spark_output.log`
4. This document: `DEPLOYMENT_FIXES.md`

